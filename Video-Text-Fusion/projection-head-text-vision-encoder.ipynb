{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(embedding_folder, has_header=True, skip_first_column=False):\n",
    "    csv_files = glob(embedding_folder + '/*.csv')\n",
    "    all_embeddings = []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "                                                                    # Read the CSV file\n",
    "        embeddings_df = pd.read_csv(csv_file, header=0 if has_header else None)\n",
    "        \n",
    "        if skip_first_column:                                       # Skip the first column for video csv files\n",
    "            embeddings_df = embeddings_df.iloc[:, 1:]               # because it contains the name of video frame\n",
    "\n",
    "    \n",
    "        embeddings_array = embeddings_df.astype(np.float64).values  # Convert all elements to numpy.float64\n",
    "                                                                    # Remove rows with NaN values\n",
    "        embeddings_array = embeddings_array[~np.isnan(embeddings_array).any(axis=1)]\n",
    "        all_embeddings.append(embeddings_array)                     # Append the NumPy array to the list\n",
    "        \n",
    "\n",
    "    # Concatenate all embeddings into a single NumPy array\n",
    "    embeddings_array = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    return embeddings_array\n",
    "\n",
    "text_embedding_folder = 'DATASET-TextEmbedding'\n",
    "vid_embedding_folder = 'DATASET-VideoEmbeddings'\n",
    "\n",
    "text_embeddings = process_csv_files(text_embedding_folder, has_header=False)\n",
    "video_embeddings = process_csv_files(vid_embedding_folder, has_header=True, skip_first_column=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33789062  0.19824219 -0.296875   ... -0.15917969  0.03417969\n",
      "   0.09179688]\n",
      " [-0.28710938  0.29296875 -0.04467773 ... -0.01049805 -0.25976562\n",
      "   0.11083984]\n",
      " [-0.06933594  0.15332031 -0.02490234 ...  0.06054688 -0.19238281\n",
      "   0.27148438]\n",
      " ...\n",
      " [-0.01116943  0.06738281  0.13867188 ...  0.18066406 -0.23730469\n",
      "  -0.17578125]\n",
      " [-0.08251953  0.12988281  0.18945312 ...  0.18164062  0.03271484\n",
      "  -0.09472656]\n",
      " [-0.0279541   0.03369141 -0.03027344 ...  0.13574219 -0.0004921\n",
      "   0.26171875]]\n",
      "[[0.96737421 0.94864553 0.93027443 ... 0.88816762 1.02650452 0.98235989]\n",
      " [0.97024786 0.97135627 0.97625911 ... 0.92484605 1.05267894 0.94103754]\n",
      " [0.95929432 0.97145921 0.97567111 ... 0.93477076 1.09334481 0.89381284]\n",
      " ...\n",
      " [0.87987137 0.9994846  1.00720835 ... 0.91209149 1.10154176 0.98179394]\n",
      " [0.86699724 1.04539263 0.90224159 ... 0.91217023 1.14227533 0.93422842]\n",
      " [0.8895027  0.96568727 0.93459833 ... 0.89999688 1.05757153 0.93650216]]\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings)\n",
    "print(video_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types in video_embeddings: {<class 'numpy.float64'>}\n"
     ]
    }
   ],
   "source": [
    "# Check data types in the video_embeddings array\n",
    "print(\"Data Types in video_embeddings:\", set(type(item) for row in video_embeddings for item in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert NumPy arrays to PyTorch tensors\n",
    "# text_embeddings_tensor = torch.tensor(text_embeddings, dtype=torch.float32)\n",
    "# video_embeddings_tensor = torch.tensor(video_embeddings, dtype=torch.float32)\n",
    "\n",
    "# # Normalize the embeddings using StandardScaler from scikit-learn\n",
    "# scaler = StandardScaler()\n",
    "# text_embeddings_normalized = scaler.fit_transform(text_embeddings)\n",
    "# video_embeddings_normalized = scaler.fit_transform(video_embeddings)\n",
    "\n",
    "# # Convert normalized NumPy arrays to PyTorch tensors\n",
    "# text_embeddings_normalized_tensor = torch.tensor(text_embeddings_normalized, dtype=torch.float32)\n",
    "# video_embeddings_normalized_tensor = torch.tensor(video_embeddings_normalized, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "text_embeddings_tensor = torch.tensor(text_embeddings, dtype=torch.float32)\n",
    "video_embeddings_tensor = torch.tensor(video_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Normalize the embeddings using StandardScaler from scikit-learn\n",
    "scaler_text = StandardScaler()\n",
    "scaler_video = StandardScaler()\n",
    "\n",
    "# Fit and transform each set of embeddings separately\n",
    "text_embeddings_normalized = scaler_text.fit_transform(text_embeddings)\n",
    "video_embeddings_normalized = scaler_video.fit_transform(video_embeddings)\n",
    "# Find the minimum dimensionality among all sets of embeddings\n",
    "min_dim = min(text_embeddings_normalized.shape[1], video_embeddings_normalized.shape[1])\n",
    "\n",
    "# Trim embeddings to the minimum dimensionality\n",
    "text_embeddings_normalized = text_embeddings_normalized[:, :min_dim]\n",
    "video_embeddings_normalized = video_embeddings_normalized[:, :min_dim]\n",
    "\n",
    "# Concatenate normalized embeddings\n",
    "all_embeddings_normalized = np.concatenate([text_embeddings_normalized, video_embeddings_normalized], axis=0)\n",
    "\n",
    "# Normalize the concatenated embeddings using a single StandardScaler from scikit-learn\n",
    "scaler_all = StandardScaler()\n",
    "all_embeddings_normalized = scaler_all.fit_transform(all_embeddings_normalized)\n",
    "\n",
    "# Split normalized embeddings back into text, video, and subtitle embeddings\n",
    "text_embeddings_normalized_tensor = torch.tensor(all_embeddings_normalized[:len(text_embeddings)], dtype=torch.float32)\n",
    "video_embeddings_normalized_tensor = torch.tensor(all_embeddings_normalized[len(text_embeddings):len(text_embeddings)+len(video_embeddings)], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if dimensions of subtitle embeddings match the dimensions of video embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings_normalized_tensor.shape[1] == video_embeddings_normalized_tensor.shape[1] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
