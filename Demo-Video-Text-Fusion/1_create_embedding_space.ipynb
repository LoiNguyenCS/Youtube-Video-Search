{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-Youtube-Video-Search\\Youtube-Video-Search\\Video-Text-Fusion\\create_embedding_space.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m text_embedding_folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDATASET-TextEmbedding\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m vid_embedding_folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDATASET-VideoEmbeddings\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m text_embeddings \u001b[39m=\u001b[39m process_csv_files(text_embedding_folder, has_header\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m video_embeddings \u001b[39m=\u001b[39m process_csv_files(vid_embedding_folder, has_header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, skip_first_column\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32md:\\GIT-Youtube-Video-Search\\Youtube-Video-Search\\Video-Text-Fusion\\create_embedding_space.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     all_embeddings\u001b[39m.\u001b[39mappend(embeddings_array)                     \u001b[39m# Append the NumPy array to the list\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Concatenate all embeddings into a single NumPy array\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m embeddings_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(all_embeddings, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-Youtube-Video-Search/Youtube-Video-Search/Video-Text-Fusion/create_embedding_space.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings_array\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "def process_csv_files(embedding_folder, has_header=True, skip_first_column=False):\n",
    "    csv_files = glob(embedding_folder + '/*.csv')\n",
    "    all_embeddings = []\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "                                                                    # Read the CSV file\n",
    "        embeddings_df = pd.read_csv(csv_file, header=0 if has_header else None)\n",
    "        \n",
    "        if skip_first_column:                                       # Skip the first column for video csv files\n",
    "            embeddings_df = embeddings_df.iloc[:, 1:]               # because it contains the name of video frame\n",
    "\n",
    "    \n",
    "        embeddings_array = embeddings_df.astype(np.float64).values  # Convert all elements to numpy.float64\n",
    "                                                                    # Remove rows with NaN values\n",
    "        embeddings_array = embeddings_array[~np.isnan(embeddings_array).any(axis=1)]\n",
    "        all_embeddings.append(embeddings_array)                     # Append the NumPy array to the list\n",
    "        \n",
    "\n",
    "    # Concatenate all embeddings into a single NumPy array\n",
    "    embeddings_array = np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    return embeddings_array\n",
    "\n",
    "text_embedding_folder = 'DATASET-TextEmbedding'\n",
    "vid_embedding_folder = 'DATASET-VideoEmbeddings'\n",
    "\n",
    "text_embeddings = process_csv_files(text_embedding_folder, has_header=False)\n",
    "video_embeddings = process_csv_files(vid_embedding_folder, has_header=True, skip_first_column=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33789062  0.19824219 -0.296875   ... -0.15917969  0.03417969\n",
      "   0.09179688]\n",
      " [-0.28710938  0.29296875 -0.04467773 ... -0.01049805 -0.25976562\n",
      "   0.11083984]\n",
      " [-0.06933594  0.15332031 -0.02490234 ...  0.06054688 -0.19238281\n",
      "   0.27148438]\n",
      " ...\n",
      " [-0.01116943  0.06738281  0.13867188 ...  0.18066406 -0.23730469\n",
      "  -0.17578125]\n",
      " [-0.08251953  0.12988281  0.18945312 ...  0.18164062  0.03271484\n",
      "  -0.09472656]\n",
      " [-0.0279541   0.03369141 -0.03027344 ...  0.13574219 -0.0004921\n",
      "   0.26171875]]\n",
      "[[0.96737421 0.94864553 0.93027443 ... 0.88816762 1.02650452 0.98235989]\n",
      " [0.97024786 0.97135627 0.97625911 ... 0.92484605 1.05267894 0.94103754]\n",
      " [0.95929432 0.97145921 0.97567111 ... 0.93477076 1.09334481 0.89381284]\n",
      " ...\n",
      " [0.87987137 0.9994846  1.00720835 ... 0.91209149 1.10154176 0.98179394]\n",
      " [0.86699724 1.04539263 0.90224159 ... 0.91217023 1.14227533 0.93422842]\n",
      " [0.8895027  0.96568727 0.93459833 ... 0.89999688 1.05757153 0.93650216]]\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings)\n",
    "print(video_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types in video_embeddings: {<class 'numpy.float64'>}\n"
     ]
    }
   ],
   "source": [
    "# Check data types in the video_embeddings array\n",
    "print(\"Data Types in video_embeddings:\", set(type(item) for row in video_embeddings for item in row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "text_embeddings_tensor = torch.tensor(text_embeddings, dtype=torch.float32)\n",
    "video_embeddings_tensor = torch.tensor(video_embeddings, dtype=torch.float32)\n",
    "\n",
    "# Normalize the embeddings using StandardScaler from scikit-learn\n",
    "scaler_text = StandardScaler()\n",
    "scaler_video = StandardScaler()\n",
    "\n",
    "# Fit and transform each set of embeddings separately\n",
    "text_embeddings_normalized = scaler_text.fit_transform(text_embeddings)\n",
    "video_embeddings_normalized = scaler_video.fit_transform(video_embeddings)\n",
    "# Find the minimum dimensionality among all sets of embeddings\n",
    "min_dim = min(text_embeddings_normalized.shape[1], video_embeddings_normalized.shape[1])\n",
    "\n",
    "# Trim embeddings to the minimum dimensionality\n",
    "text_embeddings_normalized = text_embeddings_normalized[:, :min_dim]\n",
    "video_embeddings_normalized = video_embeddings_normalized[:, :min_dim]\n",
    "\n",
    "# Concatenate normalized embeddings\n",
    "all_embeddings_normalized = np.concatenate([text_embeddings_normalized, video_embeddings_normalized], axis=0)\n",
    "\n",
    "# Normalize the concatenated embeddings using a single StandardScaler from scikit-learn\n",
    "scaler_all = StandardScaler()\n",
    "all_embeddings_normalized = scaler_all.fit_transform(all_embeddings_normalized)\n",
    "\n",
    "# Split normalized embeddings back into text, video, and subtitle embeddings\n",
    "text_embeddings_normalized_tensor = torch.tensor(all_embeddings_normalized[:len(text_embeddings)], dtype=torch.float32)\n",
    "video_embeddings_normalized_tensor = torch.tensor(all_embeddings_normalized[len(text_embeddings):len(text_embeddings)+len(video_embeddings)], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if dimensions of subtitle embeddings match the dimensions of video embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings_normalized_tensor.shape[1] == video_embeddings_normalized_tensor.shape[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, projection_dim, num_layers=2, dropout_rate=0.1):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, projection_dim))\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.ReLU(), nn.Linear(projection_dim, projection_dim), nn.Dropout(dropout_rate)])\n",
    "        self.projection = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "# Implement the vision encoder with CNN\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, projection_dim):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.projection_head = ProjectionHead(256 * 8 * 8, projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.projection_head(x)\n",
    "        return x\n",
    "\n",
    "# Implement the text encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, projection_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.projection_head = ProjectionHead(input_dim, projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate VisionEncoder, TextEncoder, and ProjectionHead\n",
    "vision_encoder = VisionEncoder(input_channels=3, projection_dim=64)\n",
    "text_encoder = TextEncoder(input_dim=min_dim, projection_dim=64)\n",
    "\n",
    "# Example usage of the encoders\n",
    "video_embedding = vision_encoder(video_embeddings_normalized_tensor)\n",
    "text_embedding = text_encoder(text_embeddings_normalized_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
