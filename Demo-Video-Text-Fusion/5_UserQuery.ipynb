{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import normalize, cosine_similarity\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TextEncoder, VisionEncoder and DualEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, projection_dim, num_layers=2, dropout_rate=0.1):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, projection_dim))\n",
    "        for _ in range(num_layers):\n",
    "            layers.extend([nn.ReLU(), nn.Linear(projection_dim, projection_dim), nn.Dropout(dropout_rate)])\n",
    "        self.projection = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, input_channels, projection_dim):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, projection_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    " \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, projection_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, projection_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, video_embeddings, text_embeddings):\n",
    "        self.video_embeddings = video_embeddings\n",
    "        self.text_embeddings = text_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_embedding = self.video_embeddings[idx]\n",
    "        text_embedding = self.text_embeddings[idx]\n",
    "\n",
    "        return {'video_embedding': video_embedding, 'text_embedding': text_embedding}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, text_encoder, vision_encoder, temperature=1.0):\n",
    "        super(DualEncoder, self).__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, captions, images):\n",
    "        # Get the embeddings\n",
    "        caption_embeddings = self.text_encoder(captions)\n",
    "        image_embeddings = self.vision_encoder(images)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # Calculate dot-product similarity for captions and images.\n",
    "        logits = torch.matmul(caption_embeddings, image_embeddings.t()) / self.temperature\n",
    "        captions_similarity = torch.matmul(caption_embeddings, caption_embeddings.t())\n",
    "        images_similarity = torch.matmul(image_embeddings, image_embeddings.t())\n",
    "        # Targets: average of similarities between captions and images.\n",
    "        targets = torch.nn.functional.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature), dim=1\n",
    "        )\n",
    "        # Compute the loss using crossentropy.\n",
    "        captions_loss = self.loss_criterion(logits, torch.argmax(targets, dim=1))\n",
    "        images_loss = self.loss_criterion(logits.t(), torch.argmax(targets, dim=0))\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def training_step(self, captions, images):\n",
    "        # Forward pass\n",
    "        caption_embeddings, image_embeddings = self(captions, images)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, captions, images):\n",
    "        caption_embeddings, image_embeddings = self(captions, images)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualEncoder(\n",
       "  (text_encoder): TextEncoder(\n",
       "    (fc_layers): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vision_encoder): VisionEncoder(\n",
       "    (fc_layers): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate VisionEncoder, TextEncoder\n",
    "vision_encoder = VisionEncoder(input_channels=300, projection_dim=512)\n",
    "text_encoder = TextEncoder(input_dim=300, projection_dim=512)\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=1.0)\n",
    "\n",
    "# Load the trained model state dictionary\n",
    "dual_encoder.load_state_dict(torch.load('final_trained_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "dual_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StringEncoder(model_name, text):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the input text and get the embeddings\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input_ids)[\"last_hidden_state\"]\n",
    "\n",
    "    # Return the embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text to Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: text embedding tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([11])\n",
      "Model input dimension: 11\n",
      "Query Embedding: tensor([-1.3131e+08, -1.7661e+08,  6.6014e+07,  2.8985e+07, -8.5384e+07,\n",
      "         2.3054e+08, -7.5165e+07, -1.2140e+08, -3.3044e+07,  3.4474e+08,\n",
      "         2.4152e+08,  2.2820e+07,  5.0775e+08, -1.4650e+08,  5.9302e+07,\n",
      "        -9.3238e+07, -1.5730e+08, -3.7881e+07, -2.7724e+08, -2.3220e+08,\n",
      "        -7.5880e+07,  6.2738e+07, -3.6293e+08,  1.3033e+08, -2.3097e+07,\n",
      "         9.3789e+07, -8.0836e+07, -6.4580e+07,  1.7577e+07,  2.8726e+07,\n",
      "        -7.0629e+07, -1.4580e+08, -2.4964e+06,  5.8642e+07,  1.5697e+08,\n",
      "        -3.0691e+07,  1.2644e+08, -8.5521e+07, -2.2248e+07, -2.6229e+08,\n",
      "        -1.7647e+08, -1.3815e+08, -9.9745e+07, -2.8437e+08, -8.3837e+07,\n",
      "        -1.7935e+07, -1.3109e+07, -1.2078e+08,  2.2371e+08, -9.7195e+07,\n",
      "        -2.2799e+08,  2.5087e+08, -1.0022e+08,  2.6720e+07, -1.8680e+08,\n",
      "         1.0164e+08,  7.0585e+07,  2.6267e+07,  2.1471e+08, -7.6310e+07,\n",
      "        -2.1280e+08, -8.7102e+07, -1.9998e+08,  2.7811e+07,  1.3001e+07,\n",
      "        -8.9904e+07, -3.4291e+07,  2.7241e+08, -1.0731e+08, -5.2748e+07,\n",
      "        -9.2722e+07,  7.8007e+07,  2.5302e+08, -1.6143e+07, -2.3629e+06,\n",
      "        -1.1110e+08, -1.9001e+08, -1.0493e+08, -4.8013e+07,  3.4426e+08,\n",
      "        -1.9822e+07,  9.5657e+06,  1.6109e+08, -1.7778e+08, -1.9868e+08,\n",
      "         1.0294e+08,  1.0252e+08, -3.7287e+07,  2.8286e+07,  3.2411e+08,\n",
      "         1.0736e+08, -1.7891e+08,  4.7053e+07,  2.2697e+08, -6.3395e+07,\n",
      "         1.2865e+08,  7.9311e+07,  1.6505e+06, -9.4480e+07, -1.1511e+08,\n",
      "        -1.3413e+08, -1.8564e+07, -2.8688e+06, -3.3548e+08,  4.5237e+07,\n",
      "         7.7552e+07, -1.6393e+08,  7.3779e+07,  1.1889e+07, -1.0267e+07,\n",
      "         4.7456e+07,  5.2324e+07, -2.1327e+08,  7.2372e+07, -1.3547e+08,\n",
      "         9.0766e+07, -3.7175e+08, -3.2116e+07,  3.6756e+07,  1.1325e+08,\n",
      "         1.2464e+08, -2.4078e+08, -1.2391e+08, -2.1175e+08,  3.4825e+08,\n",
      "        -5.9702e+07,  3.9734e+06, -9.6377e+07,  3.3011e+08,  4.0840e+07,\n",
      "         2.2762e+07,  2.4918e+07, -1.5381e+08,  6.4981e+07, -2.8364e+08,\n",
      "        -1.1756e+07,  1.2869e+08,  1.1893e+08,  6.4319e+06,  5.7842e+07,\n",
      "         1.1538e+07,  1.4073e+08,  8.3639e+07, -2.6984e+08, -1.1805e+08,\n",
      "        -8.8771e+07, -3.4848e+08,  8.6289e+07, -7.5728e+07, -9.6512e+07,\n",
      "        -2.6497e+08,  1.6155e+07, -3.3089e+08, -4.0284e+08, -1.1062e+08,\n",
      "        -1.2193e+08, -3.7573e+08, -3.4676e+08, -1.4299e+08,  2.8049e+08,\n",
      "         1.0490e+08,  3.7931e+08,  3.2629e+08,  1.6983e+07,  8.5718e+07,\n",
      "         1.0454e+08,  4.5984e+07,  1.0678e+08,  1.0788e+07,  1.8348e+08,\n",
      "        -6.8402e+07,  1.1168e+08, -2.4348e+07, -1.6443e+08, -1.6370e+08,\n",
      "        -3.6934e+06, -1.5359e+08,  9.7274e+07,  5.8756e+07,  1.4373e+08,\n",
      "         6.0611e+07, -1.9046e+06, -2.1684e+08,  9.5739e+07, -6.0090e+07,\n",
      "        -4.0979e+07, -2.0724e+08, -2.5122e+07,  1.0132e+08,  1.7149e+08,\n",
      "         1.1869e+08, -6.0864e+07, -1.3002e+08, -8.9413e+07,  8.8105e+07,\n",
      "        -1.3201e+08, -5.8574e+06,  1.2586e+08, -3.2833e+07,  6.5557e+07,\n",
      "         6.1876e+07,  7.1602e+07, -2.1301e+08, -8.2954e+07,  4.3734e+07,\n",
      "        -3.7684e+07, -1.0790e+08,  2.7217e+08,  7.4899e+07, -4.1296e+07,\n",
      "        -2.8650e+07,  1.9897e+08,  1.6015e+08,  2.5679e+07, -5.3171e+07,\n",
      "        -8.6808e+07,  1.4687e+08, -2.6714e+08,  6.5880e+07, -5.4564e+06,\n",
      "        -6.0910e+06,  1.4471e+08, -2.7748e+08, -9.8001e+07, -2.0808e+08,\n",
      "         1.4335e+08, -1.2068e+07,  6.0725e+07, -1.3309e+08,  2.1625e+08,\n",
      "        -6.4637e+07,  2.5689e+07, -1.3836e+08,  1.9361e+08, -3.5598e+08,\n",
      "        -8.1832e+07, -1.2811e+08,  2.1786e+08, -9.0587e+07,  2.6908e+08,\n",
      "         2.0659e+07, -4.9251e+07, -8.1788e+07, -9.5642e+07,  2.3034e+08,\n",
      "        -7.8256e+07,  2.9417e+08, -1.7734e+08, -2.0077e+08, -2.5312e+08,\n",
      "         4.0144e+08,  6.1695e+07, -5.3331e+07, -1.7132e+08, -9.7207e+07,\n",
      "        -1.4557e+08,  8.0789e+06,  6.5985e+07,  4.9962e+07,  1.7681e+08,\n",
      "         1.6008e+08,  4.1150e+07, -1.4457e+08,  2.3385e+07, -1.0992e+08,\n",
      "         2.6070e+07,  8.3369e+07,  2.6607e+08, -3.9081e+08, -4.8830e+07,\n",
      "        -9.1835e+07, -1.7770e+08, -1.7855e+07,  1.4635e+08,  1.7194e+08,\n",
      "         4.9818e+07,  1.3224e+08,  1.2593e+08, -6.7451e+07,  2.9195e+06,\n",
      "         1.4860e+08,  7.7870e+07,  8.1673e+07,  2.8441e+08,  2.2548e+08,\n",
      "         2.5975e+08, -6.2033e+06, -1.3901e+08,  3.4547e+07,  1.6220e+07,\n",
      "         1.0497e+08, -2.1402e+08, -1.1108e+08, -8.3039e+07, -2.0479e+08,\n",
      "        -4.7704e+07, -1.0455e+08, -2.0404e+08, -8.3652e+07, -2.7740e+07,\n",
      "         3.9319e+07,  1.2026e+08, -2.0142e+08, -1.8308e+08,  5.7767e+07,\n",
      "         8.0441e+06, -4.2135e+07, -2.1631e+08, -1.3103e+08, -1.1407e+08,\n",
      "         4.2765e+07,  1.7742e+08,  1.1495e+08, -8.7340e+06,  1.7436e+08,\n",
      "         9.5157e+07,  1.7589e+08, -1.1581e+08, -1.4542e+08, -1.0798e+08,\n",
      "         9.8109e+07,  8.0347e+07, -2.1966e+06,  3.1315e+08,  9.4357e+07,\n",
      "         1.3332e+08,  2.8345e+07,  3.9544e+07, -2.8966e+08,  1.2801e+08,\n",
      "         6.7603e+07,  7.9365e+07, -5.3276e+06,  2.0974e+07,  4.2030e+07,\n",
      "         1.3274e+08, -2.1558e+08, -1.5067e+08,  2.0071e+08,  1.6640e+08,\n",
      "         4.1797e+07, -9.3592e+07, -1.6602e+08, -4.1766e+07, -1.3509e+08,\n",
      "        -2.5518e+08,  2.8031e+07, -9.3824e+07,  1.7778e+08, -1.2339e+08,\n",
      "        -2.1833e+08, -3.8019e+08, -1.7089e+08,  2.4694e+08,  6.9916e+07,\n",
      "         3.1749e+08, -1.4561e+08,  1.2088e+08,  1.1569e+07,  8.5071e+07,\n",
      "        -8.2131e+07,  1.7935e+07,  4.7348e+07,  2.9353e+07,  1.8100e+08,\n",
      "         1.6170e+08, -1.0602e+08, -2.1945e+08,  1.5884e+08,  1.1631e+08,\n",
      "        -2.9225e+08, -9.2079e+07,  1.3044e+08,  2.7629e+08,  1.8242e+08,\n",
      "        -7.1976e+07,  4.3057e+06,  9.9033e+07,  2.1785e+08, -2.5329e+07,\n",
      "        -1.8577e+08,  1.0908e+08, -5.3079e+07,  5.8460e+07,  2.1569e+08,\n",
      "         1.3720e+08, -8.5142e+07, -1.6696e+08,  1.5106e+08, -4.0984e+07,\n",
      "         3.5344e+07, -3.8899e+08, -1.6008e+07, -7.1352e+07,  1.9196e+07,\n",
      "        -3.5741e+07,  1.8186e+08,  7.3316e+07,  1.4442e+08,  9.7914e+07,\n",
      "         1.1546e+08,  2.1130e+07, -1.7636e+07, -2.9101e+08,  1.0402e+08,\n",
      "        -2.7749e+08,  6.4606e+07,  1.4284e+08, -2.0295e+08, -2.2047e+08,\n",
      "        -3.9264e+07,  1.2891e+08,  3.6940e+08,  1.7822e+08, -1.5781e+08,\n",
      "        -1.5138e+08,  1.8568e+08, -1.3766e+08, -1.9120e+08, -1.8708e+07,\n",
      "         1.9478e+08, -7.0044e+07, -1.2969e+08,  5.9269e+07, -1.4287e+08,\n",
      "         1.1758e+08,  4.3238e+08, -3.9836e+08, -1.8330e+08, -2.1435e+08,\n",
      "         4.6539e+07, -2.4147e+08,  1.7557e+08,  3.6176e+08,  6.7640e+07,\n",
      "         3.0857e+07, -4.5324e+06,  1.6312e+08,  1.5635e+08,  2.0057e+08,\n",
      "         3.9166e+07, -1.6175e+08,  8.8706e+07,  4.0901e+07,  2.3552e+08,\n",
      "         1.8016e+07,  2.0768e+08,  2.3476e+08, -1.4992e+08, -1.0526e+08,\n",
      "        -2.1401e+07, -1.1113e+07, -1.5859e+08, -1.2921e+08, -4.3081e+07,\n",
      "         1.3296e+07,  1.1089e+08,  1.9720e+08, -1.6774e+08, -5.2657e+07,\n",
      "        -9.0349e+07, -3.8240e+07,  3.0067e+08,  2.3333e+08,  9.8566e+07,\n",
      "        -1.3726e+08,  2.7857e+08,  1.1985e+08,  6.9118e+07,  1.3214e+08,\n",
      "        -1.0977e+08,  1.5181e+07, -3.0320e+08,  8.0746e+07, -6.0892e+07,\n",
      "         4.1731e+07, -1.6656e+07,  5.7690e+07,  9.7966e+07, -1.4461e+07,\n",
      "        -2.4483e+08,  1.6511e+08, -1.3179e+08,  1.4774e+08, -1.7556e+08,\n",
      "        -5.4730e+07, -2.1503e+08, -2.3655e+08, -6.4297e+07,  2.4887e+08,\n",
      "        -8.8865e+06,  1.3348e+08,  2.9202e+08,  1.2743e+08,  2.4578e+08,\n",
      "         1.9183e+08, -2.6813e+07, -2.3605e+08, -1.2465e+08,  2.9218e+08,\n",
      "        -2.8381e+08,  1.1993e+08, -2.2664e+08, -3.4099e+07,  3.2033e+08,\n",
      "        -1.3042e+07, -2.1663e+08,  7.8771e+07, -1.1845e+08, -1.5272e+08,\n",
      "        -2.2265e+07, -1.7691e+07], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "query_text = \"Swifts Appearance Drives Up Ticket Prices For Sundays Chiefs Jets Game\"\n",
    "# Tokenize the text and convert it to a tensor\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(query_text)\n",
    "vocab = set(tokens)\n",
    "numerical_representation = torch.tensor([int(hash(token)) % (2**32 - 1) for token in tokens])\n",
    "\n",
    "# Create an instance of TextEncoder\n",
    "input_dim = len(vocab)  \n",
    "projection_dim =  512\n",
    "text_encoder = TextEncoder(input_dim, projection_dim)\n",
    "\n",
    "print(\"Input tensor shape:\", numerical_representation.shape)\n",
    "print(\"Model input dimension:\", input_dim)\n",
    "\n",
    "# Convert numerical_representation to the same dtype as the model's parameters\n",
    "numerical_representation = numerical_representation.to(dtype=text_encoder.fc_layers[0].weight.dtype)\n",
    "\n",
    "# Forward pass through the TextEncoder\n",
    "query_embedding = text_encoder(numerical_representation)\n",
    "print(\"Query Embedding:\", query_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image embedding dataframe from precomputed image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: dataframe contain all frames of all videos and their corresponding image embeddings that were previously computed using a pre-trained ResNet-18 model as an image encoder (see ***Demo-Video-Similarity-Search/create_video_embeddings.ipynb*** for demonstration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thaou\\AppData\\Local\\Temp\\ipykernel_23556\\4095948534.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, df_vid], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'DATASET-VideoEmbeddings'\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "# Load each CSV file and concatenate the DataFrames\n",
    "df = pd.DataFrame()\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df_vid = pd.read_csv(file_path)\n",
    "\n",
    "    # Add a 'Video_Name' column to identify each video\n",
    "    video_name = os.path.splitext(csv_file)[0] \n",
    "    df_vid['Video_Name'] = video_name\n",
    "    df = pd.concat([df, df_vid], ignore_index=True)\n",
    "\n",
    "# Normalize embeddings\n",
    "# Separate 'Frame_Name' column for later use\n",
    "frame_names = df['Frame_Name']\n",
    "df = df.drop(columns=['Frame_Name', 'Video_Name'])\n",
    "df = pd.DataFrame(normalize(df, axis=1), columns=df.columns)\n",
    "\n",
    "# Concatenate 'Frame_Name' column back to the DataFrame\n",
    "df['Frame_Name'] = frame_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_0</th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_503</th>\n",
       "      <th>Feature_504</th>\n",
       "      <th>Feature_505</th>\n",
       "      <th>Feature_506</th>\n",
       "      <th>Feature_507</th>\n",
       "      <th>Feature_508</th>\n",
       "      <th>Feature_509</th>\n",
       "      <th>Feature_510</th>\n",
       "      <th>Feature_511</th>\n",
       "      <th>Frame_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.045443</td>\n",
       "      <td>0.044564</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.050924</td>\n",
       "      <td>0.039351</td>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.056195</td>\n",
       "      <td>0.044337</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039878</td>\n",
       "      <td>0.043183</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>0.050661</td>\n",
       "      <td>0.047833</td>\n",
       "      <td>0.046347</td>\n",
       "      <td>0.041723</td>\n",
       "      <td>0.048221</td>\n",
       "      <td>0.046147</td>\n",
       "      <td>2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045598</td>\n",
       "      <td>0.045650</td>\n",
       "      <td>0.045880</td>\n",
       "      <td>0.050546</td>\n",
       "      <td>0.040334</td>\n",
       "      <td>0.042043</td>\n",
       "      <td>0.045416</td>\n",
       "      <td>0.056799</td>\n",
       "      <td>0.044544</td>\n",
       "      <td>0.042159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040946</td>\n",
       "      <td>0.044024</td>\n",
       "      <td>0.042006</td>\n",
       "      <td>0.055342</td>\n",
       "      <td>0.048176</td>\n",
       "      <td>0.046328</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.049472</td>\n",
       "      <td>0.044225</td>\n",
       "      <td>2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.045179</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.045950</td>\n",
       "      <td>0.050591</td>\n",
       "      <td>0.040414</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>0.043415</td>\n",
       "      <td>0.056406</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.044331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042489</td>\n",
       "      <td>0.043959</td>\n",
       "      <td>0.043274</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>0.049206</td>\n",
       "      <td>0.046792</td>\n",
       "      <td>0.044024</td>\n",
       "      <td>0.051492</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.044533</td>\n",
       "      <td>0.045341</td>\n",
       "      <td>0.044516</td>\n",
       "      <td>0.049777</td>\n",
       "      <td>0.042718</td>\n",
       "      <td>0.039888</td>\n",
       "      <td>0.043203</td>\n",
       "      <td>0.057817</td>\n",
       "      <td>0.046113</td>\n",
       "      <td>0.043583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042967</td>\n",
       "      <td>0.043936</td>\n",
       "      <td>0.043244</td>\n",
       "      <td>0.053722</td>\n",
       "      <td>0.048281</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.043002</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.044543</td>\n",
       "      <td>0.044307</td>\n",
       "      <td>0.044359</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.038902</td>\n",
       "      <td>0.042239</td>\n",
       "      <td>0.044177</td>\n",
       "      <td>0.056450</td>\n",
       "      <td>0.045668</td>\n",
       "      <td>0.043098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040940</td>\n",
       "      <td>0.044535</td>\n",
       "      <td>0.041731</td>\n",
       "      <td>0.053873</td>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.048552</td>\n",
       "      <td>0.044719</td>\n",
       "      <td>0.051249</td>\n",
       "      <td>0.043723</td>\n",
       "      <td>2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10985</th>\n",
       "      <td>0.042669</td>\n",
       "      <td>0.051468</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>0.052202</td>\n",
       "      <td>0.046959</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>0.040148</td>\n",
       "      <td>0.054444</td>\n",
       "      <td>0.045893</td>\n",
       "      <td>0.044535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041764</td>\n",
       "      <td>0.041338</td>\n",
       "      <td>0.040663</td>\n",
       "      <td>0.052616</td>\n",
       "      <td>0.048509</td>\n",
       "      <td>0.045287</td>\n",
       "      <td>0.043127</td>\n",
       "      <td>0.050034</td>\n",
       "      <td>0.047328</td>\n",
       "      <td>Zelenskyy_Addresses_The_UN_Security_Council_To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10986</th>\n",
       "      <td>0.043562</td>\n",
       "      <td>0.044194</td>\n",
       "      <td>0.047336</td>\n",
       "      <td>0.046317</td>\n",
       "      <td>0.044575</td>\n",
       "      <td>0.039825</td>\n",
       "      <td>0.045231</td>\n",
       "      <td>0.057896</td>\n",
       "      <td>0.045042</td>\n",
       "      <td>0.045179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041124</td>\n",
       "      <td>0.042022</td>\n",
       "      <td>0.041736</td>\n",
       "      <td>0.054149</td>\n",
       "      <td>0.046368</td>\n",
       "      <td>0.041673</td>\n",
       "      <td>0.042671</td>\n",
       "      <td>0.047120</td>\n",
       "      <td>0.047027</td>\n",
       "      <td>Zelenskyy_Addresses_The_UN_Security_Council_To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10987</th>\n",
       "      <td>0.042242</td>\n",
       "      <td>0.048944</td>\n",
       "      <td>0.044493</td>\n",
       "      <td>0.048755</td>\n",
       "      <td>0.046315</td>\n",
       "      <td>0.038265</td>\n",
       "      <td>0.043463</td>\n",
       "      <td>0.058381</td>\n",
       "      <td>0.045202</td>\n",
       "      <td>0.046411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042628</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>0.041210</td>\n",
       "      <td>0.055984</td>\n",
       "      <td>0.050291</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>0.044134</td>\n",
       "      <td>0.052522</td>\n",
       "      <td>0.045944</td>\n",
       "      <td>Zelenskyy_Addresses_The_UN_Security_Council_To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10988</th>\n",
       "      <td>0.043596</td>\n",
       "      <td>0.043367</td>\n",
       "      <td>0.046413</td>\n",
       "      <td>0.050501</td>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.037183</td>\n",
       "      <td>0.040676</td>\n",
       "      <td>0.058908</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.036741</td>\n",
       "      <td>0.039801</td>\n",
       "      <td>0.054503</td>\n",
       "      <td>0.049948</td>\n",
       "      <td>0.045490</td>\n",
       "      <td>0.044431</td>\n",
       "      <td>0.051799</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>Zelenskyy_Addresses_The_UN_Security_Council_To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10989</th>\n",
       "      <td>0.043675</td>\n",
       "      <td>0.044463</td>\n",
       "      <td>0.045543</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>0.045334</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>0.044135</td>\n",
       "      <td>0.056367</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>0.043882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037574</td>\n",
       "      <td>0.040327</td>\n",
       "      <td>0.039510</td>\n",
       "      <td>0.052725</td>\n",
       "      <td>0.050152</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>0.043883</td>\n",
       "      <td>0.049950</td>\n",
       "      <td>0.046828</td>\n",
       "      <td>Zelenskyy_Addresses_The_UN_Security_Council_To...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10990 rows × 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
       "0       0.045443   0.044564   0.043701   0.050924   0.039351   0.042087   \n",
       "1       0.045598   0.045650   0.045880   0.050546   0.040334   0.042043   \n",
       "2       0.045179   0.045751   0.045950   0.050591   0.040414   0.040394   \n",
       "3       0.044533   0.045341   0.044516   0.049777   0.042718   0.039888   \n",
       "4       0.044543   0.044307   0.044359   0.050740   0.038902   0.042239   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10985   0.042669   0.051468   0.046789   0.052202   0.046959   0.041746   \n",
       "10986   0.043562   0.044194   0.047336   0.046317   0.044575   0.039825   \n",
       "10987   0.042242   0.048944   0.044493   0.048755   0.046315   0.038265   \n",
       "10988   0.043596   0.043367   0.046413   0.050501   0.046416   0.037183   \n",
       "10989   0.043675   0.044463   0.045543   0.043807   0.045334   0.036987   \n",
       "\n",
       "       Feature_6  Feature_7  Feature_8  Feature_9  ...  Feature_503  \\\n",
       "0       0.044200   0.056195   0.044337   0.043860  ...     0.039878   \n",
       "1       0.045416   0.056799   0.044544   0.042159  ...     0.040946   \n",
       "2       0.043415   0.056406   0.045872   0.044331  ...     0.042489   \n",
       "3       0.043203   0.057817   0.046113   0.043583  ...     0.042967   \n",
       "4       0.044177   0.056450   0.045668   0.043098  ...     0.040940   \n",
       "...          ...        ...        ...        ...  ...          ...   \n",
       "10985   0.040148   0.054444   0.045893   0.044535  ...     0.041764   \n",
       "10986   0.045231   0.057896   0.045042   0.045179  ...     0.041124   \n",
       "10987   0.043463   0.058381   0.045202   0.046411  ...     0.042628   \n",
       "10988   0.040676   0.058908   0.047449   0.046202  ...     0.042659   \n",
       "10989   0.044135   0.056367   0.043885   0.043882  ...     0.037574   \n",
       "\n",
       "       Feature_504  Feature_505  Feature_506  Feature_507  Feature_508  \\\n",
       "0         0.043183     0.042164     0.050661     0.047833     0.046347   \n",
       "1         0.044024     0.042006     0.055342     0.048176     0.046328   \n",
       "2         0.043959     0.043274     0.054938     0.049206     0.046792   \n",
       "3         0.043936     0.043244     0.053722     0.048281     0.046948   \n",
       "4         0.044535     0.041731     0.053873     0.049166     0.048552   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10985     0.041338     0.040663     0.052616     0.048509     0.045287   \n",
       "10986     0.042022     0.041736     0.054149     0.046368     0.041673   \n",
       "10987     0.043353     0.041210     0.055984     0.050291     0.044614   \n",
       "10988     0.036741     0.039801     0.054503     0.049948     0.045490   \n",
       "10989     0.040327     0.039510     0.052725     0.050152     0.045946   \n",
       "\n",
       "       Feature_509  Feature_510  Feature_511  \\\n",
       "0         0.041723     0.048221     0.046147   \n",
       "1         0.043464     0.049472     0.044225   \n",
       "2         0.044024     0.051492     0.042095   \n",
       "3         0.043002     0.050004     0.043348   \n",
       "4         0.044719     0.051249     0.043723   \n",
       "...            ...          ...          ...   \n",
       "10985     0.043127     0.050034     0.047328   \n",
       "10986     0.042671     0.047120     0.047027   \n",
       "10987     0.044134     0.052522     0.045944   \n",
       "10988     0.044431     0.051799     0.047692   \n",
       "10989     0.043883     0.049950     0.046828   \n",
       "\n",
       "                                              Frame_Name  \n",
       "0      2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...  \n",
       "1      2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...  \n",
       "2      2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...  \n",
       "3      2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...  \n",
       "4      2000_Bodies_Recovered_After_Dam_Bursts_In_Dern...  \n",
       "...                                                  ...  \n",
       "10985  Zelenskyy_Addresses_The_UN_Security_Council_To...  \n",
       "10986  Zelenskyy_Addresses_The_UN_Security_Council_To...  \n",
       "10987  Zelenskyy_Addresses_The_UN_Security_Council_To...  \n",
       "10988  Zelenskyy_Addresses_The_UN_Security_Council_To...  \n",
       "10989  Zelenskyy_Addresses_The_UN_Security_Council_To...  \n",
       "\n",
       "[10990 rows x 513 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(-10) # Frame_Name is the last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar images from user query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query_embedding, img_dataframe, top_n=10):\n",
    "    \"\"\"\n",
    "    Perform semantic search using cosine similarity.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_embedding: The vector of the query.\n",
    "    - img_dataframe: The img_dataframe containing all vectors.\n",
    "    - top_n: Number of similar vectors to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - top_n_indices: Indices of the top N similar vectors.\n",
    "    \"\"\"\n",
    "    # Calculate cosine similarity\n",
    "    # cosine_similarities = cosine_similarity([query_embedding], img_dataframe.iloc[:, :-1].values)[0]\n",
    "    cosine_similarities = cosine_similarity([query_embedding.detach().numpy()], img_dataframe.iloc[:, :-1].values)[0]\n",
    "\n",
    "\n",
    "    # Get the indices of top N similar vectors\n",
    "    top_n_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "\n",
    "    return top_n_indices\n",
    "\n",
    "def generate_query_vector(csv_file, target_frame_name):\n",
    "    with open(csv_file, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        header = next(reader)\n",
    "\n",
    "        # Find the index of the frame_name in the header\n",
    "        frame_name_index = header.index('Frame_Name')\n",
    "\n",
    "        for row in reader:\n",
    "            frame_name = row[0]\n",
    "            if frame_name == target_frame_name:\n",
    "                frame_data = list(map(float, row[1:]))  # Convert frame_data to float\n",
    "                return frame_data\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of the top similar vectors: [1532 6279 6193 1534 3423 6530 5569 1724 6542 5573]\n"
     ]
    }
   ],
   "source": [
    "top_n_indices = semantic_search(query_embedding, df, top_n=10)\n",
    "print(f\"Indices of the top similar vectors: {top_n_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 images: ['Biden_Zelenskyy_Address_UNs_General_Assembly_frame_4300', 'Johannesburg_Building_Fire_Kills_At_Least_73_People_frame_6375', 'Johannesburg_Building_Fire_Kills_At_Least_73_People_frame_4225', 'Biden_Zelenskyy_Address_UNs_General_Assembly_frame_4350', 'Former_Proud_Boys_Leader_Enrique_Tarrio_To_Be_Sentenced_Today_frame_3925', 'Manhunt_Continues_For_Suspect_In_Maine_Shootings_frame_0025', 'Israel_Gives_1M_Civilians_In_Northern_Gaza_24_Hours_To_Evacuate_frame_0200', 'Blinken_Announces_1_Billion_In_New_Aid_To_Ukraine_frame_3375', 'Manhunt_Continues_For_Suspect_In_Maine_Shootings_frame_0325', 'Israel_Gives_1M_Civilians_In_Northern_Gaza_24_Hours_To_Evacuate_frame_0300']\n"
     ]
    }
   ],
   "source": [
    "def get_images_from_indices(indices, img_dataframe):\n",
    "    return img_dataframe.iloc[indices]['Frame_Name'].tolist()\n",
    "\n",
    "# Retrieve the top N images\n",
    "top_n_images = get_images_from_indices(top_n_indices, df)\n",
    "\n",
    "print(f\"Top 10 images: {top_n_images}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
